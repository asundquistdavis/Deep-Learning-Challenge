{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a39e0ee",
   "metadata": {},
   "source": [
    "# Part 4: Writen Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8e093",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this project we have looked at data including applicants to the Alphabet Soup Charity with the goal of producing a model to predict which applicants wiull be successful. To create the model, varrious methods for cleaning and transforming the data were applied so that the data would work with tensorflow. After obtaining a model, several other steps where made to try and improve the model. Overall, a model to predcit an applicants outcome was made with some success: accuracy of 73.74%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464e751",
   "metadata": {},
   "source": [
    "## Results\n",
    "The model attempts to predict applicant success. The model uses the following variables to make those predictions:\n",
    "- Application Type\n",
    "- Classification\n",
    "- Affiliation\n",
    "- Use Case\n",
    "- Status\n",
    "- Income Amount\n",
    "- Special Considerations\n",
    "- Ask Amount\n",
    "\n",
    "The following variables were removed from the original dataset since they likely would not provide and predictive power:\n",
    "\n",
    "- Name\n",
    "- EIN\n",
    "\n",
    "The original model, which scored an accuracy of 72.98%, used 2 hidden layers, one with 50 nodes and the other with 10 nodes and both with a RELU activation. The use of 50 and 10 nodes is to keep the model relatively small with still having around the same order of magnitude of trainable parameters on each layer. The RELU activation is used because it typically trians the data fast and is a good starting point. \n",
    "\n",
    "The model did not acheive a performance above 75% even after tuning the parameters.\n",
    "\n",
    "The following steps where made to optimze the model:\n",
    "\n",
    "- Add more layers\n",
    "- Add more nodes\n",
    "- Change activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ced93d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The best model to predict applicant success obtained an accuracy of 73.48% using 150 nodes on two hidden layers with Relu activation. While this does not beat the goal of 75%, it does come close and it seems like the changing parameters could only make marginal improvements so a model with much higher accuracy is probably not possible. Another avenue to explore is cleaning and/or filtering the data more, however this was not done in this analyis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e891c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
